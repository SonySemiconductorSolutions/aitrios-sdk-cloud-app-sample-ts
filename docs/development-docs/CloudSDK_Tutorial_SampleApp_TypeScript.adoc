= Cloud SDK pass:[<br/>] Sample Application pass:[<br/>] TypeScript pass:[<br/>] Tutorial pass:[<br/>] 
:sectnums:
:sectnumlevels: 1
:author: Copyright 2023 Sony Semiconductor Solutions Corporation
:version-label: Version 
:revnumber: x.x.x
:revdate: YYYY - MM - DD
:trademark-desc1: AITRIOS™ and AITRIOS logos are the registered trademarks or trademarks
:trademark-desc2: of Sony Group Corporation or its affiliated companies.
:toc:
:toc-title: TOC
:toclevels: 1
:chapter-label:
:lang: en

== Change history

|===
|Date |What/Why 

|2022/12/12
|Initial draft

|2023/1/30
|Fixed typos + 
Unified the swinging of expressions + 
Fixed the notation + 
Fixed the text size of pictures + 
Updated the PDF build environment +
Changed the description of the [config.ts] in the sample application repository structure +
Changed the procedure for setting the connection information in the "Prepare to run the sample application" +
Changed implementation description for each use case for "**Cloud SDK**" 0.2.0

|2023/5/26
|Fixed the "Reference materials" in "Sample application repository structure" with FlatBuffers version up +
Fixed a lack of js-yaml in the "Package (framework) on which the sample application depends" +
Fixed a code misquote in the "Implementation description for each use case" + 
Fixed parenthesis notation for tool names + 
Added alternate text to images

|2023/12/22
|Console Developer Edition and Console
Enterprise Edition support 
|===

== Introduction
This tutorial explains a sample application using the "**Cloud SDK**". + 
This sample application demonstrates the basic use of the "**Cloud SDK**". + 
In the sample application, one can verify how to control edge AI devices using "**Cloud SDK**" and also how to check the output of the Edge AI devices uploaded to "**Console for AITRIOS**" (hereafter, referred to as "**Console**") or "Azure Blob Storage" or "Local Storage".  + 
However, Codespaces cannot be used when checking the inference results uploaded to the "Local Storage". 

[#_precondition]
== Prerequisite
=== Connection information
To use the sample application, you need connection information to access the "**Console**" from the application. + 
The acquired information is used in <<#_Execute_sampleapp,"1.Prepare to run the sample application">>. + 
The required information is as follows:

* Client application details
- When "**Console Developer Edition**" is used
** Refer to the client application list in "**Portal for AITRIOS**" or register the client application for
the sample application based on the requirement and obtain the following information. For
details, refer to "Issuing a Client Secret for SDK" in the https://developer.aitrios.sony-semicon.com/documents/portal-user-manual["**Portal User Manual**"].
*** Client ID
*** Secret
+
** Get the following information from link:++https://developer.aitrios.sony-semicon.com/en/file/download/rest-api-authentication++[this material].
*** Console endpoint
*** Portal authorization endpoint

- When using "**Console Enterprise Edition**"
** Please contact "**Console**" deployment representative (Service Administrator).


For using "Azure Blob Storage", connection information is required to access "Azure Blob Storage". + 
For details, refer https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string#configure-a-connection-string-for-an-azure-storage-account["Configure a connection string for an Azure storage account"].

=== Edge AI devices
In order for the sample application to work properly, the edge AI device must have specific settings. + 
Required settings are as follows:

* AI model and application are deployed
* AI model based on object detection is deployed
* From the "**Console**" UI, set the Command Parameter File to be used to the following setting:
+
** When using "**Console**" +
When not described, the following values are set automatically. +
UploadMethod="BlobStorage" +
UploadMethodIR="Mqtt" +
** "Azure Blob Storage" +
UploadMethod="BlobStorage" +
UploadMethodIR="BlobStorage" +
** "Local Storage" +
UploadMethod="HTTPStorage" +
UploadMethodIR="HTTPStorage"

** Other parameters need to be changed depending on the AI model and application content

=== External transfer settings
* When using "Azure Blob Storage" +
When using "Azure Blob Storage", complete the settings available in https://developer.aitrios.sony-semicon.com/en/edge-ai-sensing/documents/external-transfer-settings-tutorial-for-azure-blob-storage[External transfer setting tutorial(Azure Blob Storage)]. +
* When using "Local Storage" +
When using "Local Storage", complete the settings available in the https://developer.aitrios.sony-semicon.com/en/edge-ai-sensing/documents/external-transfer-settings-tutorial-for-http-server[External transfer settings tutorial(Local HTTP Server)].
+
IMPORTANT: Uploads from the device to HTTP Server are not encrypted due to HTTP communication.

== Sample application operating environment
See the https://developer.aitrios.sony-semicon.com/en/downloads#sdk-getting-started["**SDK Getting Started**"].

== Sample application functional overview
The sample application implements the functionality required to specify an edge AI device enrolled in the "**Console**" and get inference results and images. + 
The following three functions are implemented:

* Get information about edge AI devices enrolled in the "**Console**"
* Instruct edge AI devices to start/stop inference
+
By starting inference, the EdgeAI device uploads the inference results/images to either the "**Console**" or "Azure Blob Storage" or "Local Storage".
* Obtain the inference results/images and display the fetched results.
+
Displays the data uploaded to "**Console**" or "Azure Blob Storage" or "Local Storage".


== Sample application repository structure
Sample application operating environment is as follows: + 
Omit parts that are not relevant to the implementation.
----
aitrios-sdk-cloud-app-sample-ts
├── src (1)
│   ├── common
│   │   └── config.ts (2)
│   │   └── settings.ts (3)
│   ├── components (4)
│   │   ├── Button
│   │   │   └── index.tsx
│   │   └── DropDownList
│   │       └── index.tsx
│   ├── hooks
│   │   └── getAzureStorage.ts (5)
│   │   └── getConsoleStorage.ts (6)
│   │   └── getLocalStorage.ts (7)
│   │   └── getStorageData.ts (8)
│   │   └── useInterval.js (9)
│   ├── next-env.d.ts
│   ├── pages
│   │   ├── api
│   │   │   ├── getCommandParameterFile.ts (10)
│   │   │   ├── getDeviceData.ts (11)
│   │   │   ├── getImageAndInference.ts (12)
│   │   │   ├── startUpload.ts (13)
│   │   │   └── stopUpload.ts (14)
│   │   ├── _app.tsx (15)
│   │   └── index.tsx (16)
│   ├── public
│   │   ├── favicon.ico (17)
│   │   └── label.json (18)
│   ├── styles
│   │   ├── globals.css (19)
│   │   └── Home.module.css (20)
│   ├── tsconfig.json (21)
│   └── util
│   │   ├── bounding-box.ts (22)
│   │   ├── bounding-box2d.ts (23)
│   │   ├── general-object.ts (24)
│   │   ├── object-detection-data.ts (25)
│   │   ├── object-detection-top.ts (26)
│   │   └── sample.ts (27)
│   └── checkLocalRoot.ts (28)
│   └── tsconfig.node.json  (29)
----
(1) src : Sample application folder +
(2) config.ts : Logic to fetch the information to connect to either the "**Console**" or "Azure Blob Storage". +
(3) settings.ts : Specify the path of "Local Storage" and connection destination +
(4) components : Button / DropDownList component logic storage folder +
(5) getAzureStorage.ts : Logic to fetch the inference results and images from "Azure Blob Storage" +
(6) getConsoleStorage.ts : Logic to fetch the inference results and images from "**Console**"  +
(7) getLocalStorage.ts : Logic to fetch the inference results and images from "Local Storage" +
(8) getStorageData.ts : Determines the Storage to use and invokes the logic of the appropriate Storage +
(9) useInterval.js : Interval logic +
(10) getCommandParameterFile.ts : Logic to get parameters for the edge AI device +
(11) getDeviceData.ts : Logic to get information about edge AI devices enrolled in the "**Console**" +
(12) getImageAndInference.ts : Logic to obtain the inference results and images +
(13) startUpload.ts : Logic to start inference +
(14) stopUpload.ts : Logic to stop inference +
(15) _app.tsx : Initializing the sample application frontend +
(16) index.tsx : Sample application frontend UI +
(17) favicon.ico : Symbol icons for the sample application +
(18) label.json : Inference result display label +
(19) globals.css : Sample application frontend style sheet +
(20) Home.module.css : Sample application frontend style sheet +
(21) tsconfig.json : Compiler configuration file +
(22) bounding-box.ts : Source code to deserialize +
(23) bounding-box2d.ts : Source code to deserialize +
(24) general-object.ts : Source code to deserialize +
(25) object-detection-data.ts : Source code to deserialize +
(26) object-detection-top.ts : Source code to deserialize +
(27) sample.ts : TypeScript logic running in the sample application frontend UI +
(28) checkLocalRoot.ts : Verify the LOCAL_ROOT settings +
(29) tsconfig.node.json : Compile settings file

=== Source code commentary

The following figure provides an overview of the sample application:

image::diagram_ts.png[alt="Overview of the sample application", width="400", align="center"]

The sample application consists of the Next.js framework.

Call the "**Cloud SDK**" from the sample application to control the edge AI device through the "**Console**". +
The data obtained by the edge AI device is saved either in the "**Console**", "Azure Blob Storage" or "Local Storage". +
The sample application uses "**Cloud SDK**" and fetches the data from either the "**Console**" or "Azure Blob Storage" or "Local Storage".

=== Package (framework) on which the sample application depends

* "**Console Access Library**"
* https://nodejs.org/en/download/[node]
* https://github.com/axios/axios[axios]
* https://github.com/acode/lib-node[lib]
* https://nextjs.org/[next]
* https://reactjs.org/[react]
* https://reactjs.org/docs/react-dom.html[react-dom]
* https://google.github.io/flatbuffers/[FlatBuffers]
* https://github.com/nodeca/js-yaml[js-yaml]
* https://github.com/Azure/azure-sdk-for-js[azure-sdk-for-js]


[#_Execute_sampleapp]
== How to run the sample application
Use the connection information prepared in the <<#_precondition,"Prerequisite">>

=== 1.Prepare to run the sample application
. In Codepaces or in an environment where the repository is cloned, create
[console_access_settings.yaml] under [src/common] and set the connection destination information.

- When "**Console Developer Edition**" is used
+
|===
|src/common/console_access_settings.yaml
a|
[source, Yaml]
----
console_access_settings:
  console_endpoint: "Console endpoint"
  portal_authorization_endpoint: "Portal authorization endpoint"
  client_secret: "Secret"
  client_id: "Client ID"
----
|===
* Specify the Console endpoint in the `**console_endpoint**`. +
* Specify the Portal authentication endpoint in `**portal_authorization_endpoint**`. +
* Specify the Secret of the registered application in `**client_secret**`. +
* Specify the Client ID of the registered application in the `**client_id**`. +
+

IMPORTANT: For details on how to obtain the Client ID and Secret, please refer "Issue the Client Secret for SDK" in the https://developer.aitrios.sony-semicon.com/documents/portal-user-manual["**Portal User Manual**"]. +
For details on how to obtain the Console endpoint and the Portal authentication endpoint, please refer to link:++https://developer.aitrios.sony-semicon.com/file/download/rest-api-authentication++[this document]. +
This is the information to access the "Console". +
Do not disclose it to the public or share it with others and handle it with caution.
+

NOTE: When executing the sample application in a Proxy environment set the
environment variable `**https_proxy**`. +

- When "**Console Enterprise Edition**" is used
+ 
|===
|src/common/console_access_settings.yaml
a|
[source,Yaml]
----
console_access_settings:
  console_endpoint: "Console endpoint"
  portal_authorization_endpoint: "Portal authentication endpoint" 
  client_secret: "Secret"
  client_id:  "Client ID" 
  application_id: "Application ID"
----
|===
+ 

* Specify the Console endpoint in `**console_endpoint**`. +
* Specify the Portal authentication endpoint in `**portal_authorization_endpoint**`. +
The Portal authentication endpoint is to be specified in a `**\https://login.microsoftonline.com/{tenantID}**` format. +
* Specify the Secret of the registered application in `**client_secret**` . +
* Specify the Client ID of the registered application in the `**client_id**`. +
* Specify the Application ID of the registered application in `**application_id**`.
+

IMPORTANT: For details on how to fetch the Console endpoint, Client ID, Secret and Tenant ID and Application ID, please contact "**Console**" deployment representative (Service Administrator). +
Do not disclose it to the public or share it with others, handle it with care. +
+

NOTE: When executing the sample application in a Proxy environment, set the
environment variable `**https_proxy**`.

. In Codepaces or in an environment where the repository is cloned, create
[azure_access_settings.yaml] under [src/common] and set the connection destination information. +
This setting is set when the destination to obtain the inference results is "Azure Blob Storage". 

+ 
|===
|src/common/azure_access_settings.yaml
a|
[source,Yaml]
----
azure_access_settings:
  connection_string: "Connection information"
  container_name: "Container name".
----
|===
+ 

* Specify the Connection information of "Azure Blob Storage" in `**connection_string**`.
* Specify the Container name of "Azure Blob Storage" in `**container_name**`.

+ 
IMPORTANT: This is the information to access the "Azure Blob Storage". +
Do not disclose it to the public or share it with others and handle it
with caution.


. In Codepaces or in an environment where the repository is cloned, set the connection destination information in [settings.ts] under [src/common].

+ 
|===
|src/common/settings.ts
a|
[source,TypeScript]
----
export const SERVICE = {
  Console: 'console',
  Azure: 'azure',
  Local: 'local'
} as const
type SERVICE_TYPE = typeof SERVICE[keyof typeof SERVICE];

export const CONNECTION_DESTINATION: SERVICE_TYPE = SERVICE.Console
export const LOCAL_ROOT = ''
----
|===
+ 

* Set the destination to obtain the inference result in `**CONNECTION_DESTINATION**`. +
The default value is the `**SERVICE.Console**` setting.

* Specify the path for "Local Storage" `**LOCAL_ROOT**`.
This setting is used when `**SERVICE.Local**` is specified in `**CONNECTION_DESTINATION**`.

+ 
NOTE: When using Dev Container environment, create a folder in the folder where Local Storage is
git cloned and set `**LOCAL_ROOT**` to `**workspace/{ folder that is created within a git cloned folder}**`.

image::prepare_ts.png[alt="Prepare to run the sample application", width="700", align="center"]

=== 2.Launch the sample application
Install the package and launch the sample application from either the terminal in the environment where the repository is cloned or from Codepaces.
 
....
$ npm install
$ npm run build
$ npm run start
....

image::launch_app_ts.png[alt="Launch the sample application", width="700", align="center"]

=== 3.Start inference
Access the sample application from the browser and perform various operations.

. Open http://localhost:3000 (port forwarded URL in the case of Codepaces).
. Select a Device ID from the list of [**DeviceID**]
. Click the [**START**] to start inference for the edge AI device

image::start_inference_ts.png[alt="Start inference", width="700", align="center"]

=== 4.Review inference results and images
While inference is starting, the "**Image/Inference**" area displays an image and inference results.

image::running_ts.png[alt="Review inference results and images", width="700", align="center"]


=== 5.Stop inference
Click the [**STOP**] in the sample application to stop inference for the edge AI device.

image::stop_inference_ts.png[alt="Stop inference", width="700", align="center"]

== Implementation description for each use case

=== 1.Get information about edge AI devices enrolled in the "**Console**"

To use the "**Console**", generate a Client for the "**Cloud SDK**". + 
Use the functions provided by the "**Console**" from the generated Client.

* Import library 
+
[source, TypeScript]
----
import { Client, Config } from 'consoleAccessLibrary'
----
+
Import the libraries required for "**Cloud SDK**" client generation, as preceding.

* "**Cloud SDK**" client generation
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
----
In the preceding source code, generate the client for the "**Cloud SDK**". + 
Specify the connection information to the `**Config**` and generate the `**config**`. + 
Specify the `**config**` to the `**Client**` and generate the `**client**`.

* Get device information
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const res = await client?.deviceManagement?.getDevices(queryParams)
----
In the preceding example, get information about the enrolled edge AI devices from the "**Console**". + 
Use the generated client and run the `**getDevices**` provided by the `**deviceManagement**` to get device information. + 
Optionally acquisition conditions is configurable to the `**queryParams**`.

* Get device parameters
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const res = await client?.deviceManagement?.getCommandParameterFile()
----
Generate the `**client**` as preceding. + 
Get device parameters using the `**getCommandParameterFile**` provided by the `**deviceManagement**` of the `**client**`.

=== 2.Instruct the edge AI devices to start inference

* Start inference
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const res = await client?.deviceManagement?.startUploadInferenceResult(deviceId)
----
Generate the `**client**` as preceding. + 
Start inference using the `**startUploadInferenceResult**` provided by the `**deviceManagement**` of the `**client**`.

=== 3.Get inference results and images from the "**Console**"
Use the functionality provided by client to get inference results and images from the "**Console**".

* Get an image list
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const imageData = await client?.insight?.getImages(deviceId, subDirectoryName, numberOfImages, skip, orderBy)
----
Generate the `**client**` as preceding. + 
Get the image list using the `**getImages**` provided by the `**insight**`.

* Get the latest image and link it to the inference result
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const orderBy = 'DESC'
const numberOfImages = 1
const skip = 0
const imageData = await client?.insight?.getImages(deviceId, outputSubDir, numberOfImages, skip, orderBy)
const latestImage = imageData.data.images[0]
const ts = (latestImage.name).replace('.jpg', '')
const base64Img = `data:image/jpg;base64,${latestImage.contents}`
----
The preceding source code gets the latest image information from an image list. + 
Get the latest image data into the `**base64Img**`. + 
Get the timestamp of the latest image into the `**ts**`. + 
Inference results and images are linked by their respective timestamps. + 
Call the function to get inference results linked to images using the `**ts**`.

* Get inference results linked to the latest image
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret, application_id)
const client = await Client.createInstance(config)
const filter = `EXISTS(SELECT VALUE i FROM i IN c.Inferences WHERE i.T >= "${startTime}" AND i.T <= "${endTime}")`
const NumberOfInferenceResults = 1
const raw = 1
const time = undefined
const resInference = await client.insight.getInferenceResults(deviceId, filter, numberOfInferenceResult, raw, time)
----
Generate the `**client**` as preceding. + 
Get the list of inference results using the `**getInferenceresults**` provided by the `**insight**`. + 
`**filter**` is the argument that specifies a search filter. + 
Specify the number of inference results to get by the `**NumberOfInferenceresults**`. + 
`**raw**` is the argument for accessing the stored inference result. + 
Specify the timestamp of inference results to get by the `**time**`.

* Deserialize inference results
+
[source, TypeScript]
----
const deserializedInferenceData = deserialize(inferenceData)
----
The preceding source code converts the inference results gotten from the "**Console**" into a format that can be referenced. + 
See the https://github.com/SonySemiconductorSolutions/aitrios-sdk-deserialization-sample["Cloud SDK Deserialize Sample"] for details of this conversion process.


=== 4.Obtain the inference results and images of "Azure Blob Storage"

In order to obtain the inference results and images from "Azure Blob Storage", use
getAzureStorage.ts available in the hooks directory.

* Obtain the image list
+
[source,TypeScript]
----
export async function getImageFromAzure (deviceId: string, subDirectory: string, orderBy?: string, skip?: number, numberOfImages?: number) {
  const containerClient = getBlobService()
  const blobNames = []
  const prefix = `${deviceId}/image/${subDirectory}/`
  orderBy = orderBy || 'ASC' // ASC is cal default value
  skip = skip || 0 // 0 is cal default value
  numberOfImages = numberOfImages || 50 // 50 is cal default value
  for await (const blob of containerClient.listBlobsFlat({ prefix })) {
    blobNames.push(blob.name)
  }
  if (orderBy === 'DESC') {
    blobNames.reverse()
  }

  const images = []
  for (let i = 0; i < blobNames.length; i++) {
    if (i === numberOfImages) break
    if (blobNames[i + skip] === undefined) {
      break
    }
    const blockBlobClient = containerClient.getBlockBlobClient(blobNames[i + skip])
    const buffer = await blockBlobClient.downloadToBuffer()
    images.push({
      name: blobNames[i + skip].split('/')[3],
      contents: buffer.toString('base64')
    })
  }

  const response = {
    total_image_count: blobNames.length,
    images
  }
  return response
}
----

Obtain the list of image file names using `**listBlobsFlat**` provided by `**azure-sdk-for-js**`. +
Obtain image data by using `**getBlockBlobClient**` and `**downloadToBuffer**` by using `**azure-sdk-for-js**`.  +
Creates an image file name, base64, and returns it together with `**total_image_count**`

* Obtains the inference result associated with the latest image
+
[source,TypeScript]
----
export async function getInferenceFromAzure (retryCount: number, deviceId: string, subDirectory: string, startInferenceTime?: string, endInferenceTime?: string, numberOfInferenceResult?: number): Promise<string[]> {
  const serializeDatas: string[] = []
  if (retryCount === 0) {
    return serializeDatas
  }
  const containerClient = getBlobService()
  const blobs = []
  numberOfInferenceResult = numberOfInferenceResult || 20 // 20 is cal default value
  const prefix = `${deviceId}/metadata/${subDirectory}/`

  for await (const blob of containerClient.listBlobsByHierarchy('/', { prefix })) {
    const filePath = blob.name
    const timestamp = filePath.split('/')[3].replace('.txt', '')
    if ((startInferenceTime === undefined || timestamp >= startInferenceTime) &&
      (endInferenceTime === undefined || timestamp < endInferenceTime)) {
      blobs.push(blob.name)
    } else if (endInferenceTime !== undefined && timestamp > endInferenceTime) {
      break
    }
    if (blobs.length === numberOfInferenceResult) break
  }

  if (!(blobs.length === 0)) {
    for (let i = 0; i < blobs.length; i++) {
      const blobClient = containerClient.getBlobClient(blobs[i])
      const blobInferenceResponse = await blobClient.download(0)
      const inferenceText = await streamToString(blobInferenceResponse.readableStreamBody)
      const inferenceJson = JSON.parse(inferenceText)
      serializeDatas.push(inferenceJson)
    }
    return serializeDatas
  } else {
    await setTimeout(1000)
    return getInferenceFromAzure(retryCount - 1, deviceId, subDirectory, startInferenceTime, endInferenceTime, numberOfInferenceResult)
  }
}
----

Obtains the list of inference result file names using `**listBlobsByHierarchy**` provided by `**azure-sdk-for-js**`. +
Check whether the time stamp of the obtained inference result file name is within the specified
range. +
Obtain the inference result data using `**getBlobClient**` or `**download**` provided by `**azure-sdk-for-js**`. +
`**startInferenceTime**` is a time stamp that indicates the search start position. +
`**endInferenceTime**` is a time stamp that indicates the search end position. +
`**numberOfInferenceResult**` is the number of inference results to be obtained. 

=== 5.Obtains the inference results and images of "Local Storage"
In order to obtain the inference results and images from "Local Storage", use LocalStorage.ts
available in the hooks directory.

* Obtain the image list
+
[source,TypeScript]
----
export function getImageFromLocal (deviceId: string, subDirectory: string, orderBy?: string, skip?: number, numberOfImages?: number) {
  const storagePath = path.join(LOCAL_ROOT, deviceId, 'image', subDirectory)
  isRelativePath(storagePath)
  orderBy = orderBy || 'ASC' // ASC is cal default value
  skip = skip || 0 // 0 is cal default value
  numberOfImages = numberOfImages || 50 // 50 is cal default value
  const images: any = []
  isStoragePathFile(storagePath)
  const files = fs.readdirSync(storagePath)
  const imagesFiles = files.filter(file => {
    const extension = path.extname(file).toLowerCase()
    return extension === '.jpg'
  })
  if (orderBy === 'DESC') {
    imagesFiles.reverse()
  }
  for (let i = 0; i < numberOfImages; i++) {
    if (imagesFiles[i + skip] === undefined) {
      break
    }
    const filePath = path.join(storagePath, imagesFiles[i + skip])
    isRelativePath(filePath)
    isSymbolicLinkFile(filePath)
    const data = fs.readFileSync(filePath)
    const base64Image = base64.fromByteArray(data)
    images.push({
      name: imagesFiles[i + skip],
      contents: base64Image
    })
  }
  const response = {
    total_image_count: imagesFiles.length,
    images
  }
  return response
}
----

Obtains the list of image file names using `**readdirSync**` provided by `**fs**`. +
Obtains the image data using `**readFileSync**` provided by `**fs**`. +
Creates an image file name, base64, and returns it together with `**total_image_count**`. +

* Obtains the inference result associated with the latest image +
+
[source,TypeScript] 
----
export function getInferenceFromLocal (deviceId: string, subDirectory: string, startInferenceTime?: string, endInferenceTime?: string, numberOfInferenceResult?: number) {
  const storagePath = path.join(LOCAL_ROOT, deviceId, 'meta', subDirectory)
  isRelativePath(storagePath)
  numberOfInferenceResult = numberOfInferenceResult || 20 // 20 is cal default value
  isStoragePathFile(storagePath)
  const serializeDatas: string[] = []
  const inferencesFiles = fs.readdirSync(storagePath) // get inferences
  for (const fileName of inferencesFiles) {
    const timestamp = path.basename(fileName, '.txt')
    if ((startInferenceTime === undefined || timestamp >= startInferenceTime) &&
      (endInferenceTime === undefined || timestamp < endInferenceTime)) {
      const inferenceFilePath = path.join(LOCAL_ROOT, deviceId, 'meta', subDirectory, fileName)
      isSymbolicLinkFile(inferenceFilePath)
      const inferenceData = fs.readFileSync(inferenceFilePath, 'utf8')
      const json = JSON.parse(inferenceData)
      serializeDatas.push(json)
    } else if (endInferenceTime !== undefined && timestamp > endInferenceTime) {
      break
    }
    if (serializeDatas.length === numberOfInferenceResult) break
  }

  return serializeDatas
}
----

Obtains the list of inference result file names using `**readdirSync**` provided by `**fs**`. +
Check whether the time stamp of the obtained inference result file name is within the specified range. +
Obtains the inference result data using `**readFileSync**` provided by `**fs**`. +
`**startInferenceTime**` is a time stamp that indicates the search start position. +
`**endInferenceTime**` is a time stamp that indicates the search end position. +
`**numberOfInferenceResult**` is the number of inference results to be obtained.

=== 6.Instruct the edge AI devices to stop inference

* Stop inference
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const res = await client?.deviceManagement?.stopUploadInferenceResult(deviceId)
----
To stop inference of the edge AI device, run the `**stopUploadInferenceResult**` provided by the `**deviceManagement**` of the `**client**` as preceding. + 
Specify the Device ID to stop by the `**deviceId**`.

== Reference materials

=== Display gotten inference results (Sample application display processing)

[source, JavaScript]
----
type InferenceItem = {
  'class_id': number, // Index of the object label
  'score': number,    // Confidence of the object label
  'left': number,     // X-coordinate start position of the object
  'top': number,      // Y coordinate start position of the object
  'right': number,    // X-coordinate end position of the object
  'bottom': number    // Y coordinate end position of the object
}
const drawBoundingBox = (image, inferenceData, context, labels) => {
  if (context !== null) {
    const img = new window.Image()
    img.src = image
    img.onload = () => {
      const canvas = document.getElementById('canvas') as HTMLCanvasElement
      canvas.width = img.width
      canvas.height = img.height
      context.drawImage(img, 0, 0)

      // Display gotten inference results
      for (const [key, value] of Object.entries(inferenceData)) {
        if (key === 'T') continue
        const v = value as InferenceItem
        context.lineWidth = 3
        context.strokeStyle = 'rgb(255, 255, 0)'

        // Specify bounding box coordinates
        context.strokeRect(v.left, v.top, Math.abs(v.left - v.right), Math.abs(v.top - v.bottom))

        // Specify coordinates to display labels
        const labelPointX = (v.right > 270 ? v.right - 70 : v.right)
        const labelPointY = (v.bottom > 300 ? v.bottom - 10 : v.bottom)

        context.font = '20px Arial'
        context.fillStyle = 'rgba(255, 255, 0)'

        // Display the label and confidence
        context.fillText(`${labels[v.class_id]} ${Math.round(v.score * 100)}%`, labelPointX, labelPointY)
      }
    }
  }
}
----

* Format of image paths taken as an image list
+
----
<blobcontainer_name>/<deviceId>/JPG/<subDirectoryName>/YYYYMMDDHHMMSSFFF.jpg
----
* Sample data of inference result (object detection) + 
Inferences[] is the inference result + 
In the following sample data, there are two object detections + 
The detection results are serialized, but the following sample data is in deserialized data format.
+
[source, Json]
----
{
    "DeviceID": "123456789ABC",
    "ModelID": "0000000000000000",
    "Image": true,
    "Inferences": [
        {
            "1": {
                "class_id": 18,
                "score": 0.03125,
                "left": 8,
                "top": 0,
                "right": 303,
                "bottom": 107
            },
            "2": {
                "class_id": 19,
                "score": 0.02734375,
                "left": 2,
                "top": 230,
                "right": 38,
                "bottom": 319
            },
            "T": "20220101010101000"
        }
    ],
    "id": "00000000-0000-0000-0000-000000000000",
    "_rid": "AAAAAAAAAAAAAAAAAAAAAA==",
    "_self": "dbs/XXXXXX==/colls/CCCCCCCCCCCC=/docs/AAAAAAAAAAAAAAAAAAAAAA==/",
    "_etag": "\"00000000-0000-0000-0000-000000000000\"",
    "_attachments": "attachments/",
    "_ts": 1111111111
}
----
+
The parameters of the detection result are as follows:
+
class_id: Index of the object label
+
score: Confidence of the object label
+
left: X-coordinate start position of the object
+
top: Y coordinate start position of the object
+
right: X-coordinate end position of the object
+
bottom: Y coordinate end position of the object
