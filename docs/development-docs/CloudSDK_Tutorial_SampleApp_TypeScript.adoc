= Cloud SDK pass:[<br/>] Sample Application pass:[<br/>] TypeScript pass:[<br/>] Tutorial pass:[<br/>] 
:sectnums:
:sectnumlevels: 1
:author: Copyright 2023 Sony Semiconductor Solutions Corporation
:version-label: Version 
:revnumber: x.x.x
:revdate: YYYY - MM - DD
:trademark-desc1: AITRIOS™ and AITRIOS logos are the registered trademarks or trademarks
:trademark-desc2: of Sony Group Corporation or its affiliated companies.
:toc:
:toc-title: TOC
:toclevels: 1
:chapter-label:
:lang: en
:imagesdir: ./images/

== Change history

|===
|Date |What/Why 

|2022/12/12
|Initial draft

|2023/1/30
|Fixed typos + 
Unified the swinging of expressions + 
Fixed the notation + 
Fixed the text size of pictures + 
Updated the PDF build environment +
Changed the description of the [config.ts] in the sample application repository structure +
Changed the procedure for setting the connection information in the "Prepare to run the sample application" +
Changed implementation description for each use case for "**Cloud SDK**" 0.2.0

|2023/5/26
|Fixed the "Reference materials" in "Sample application repository structure" with FlatBuffers version up +
Fixed a lack of js-yaml in the "Package (framework) on which the sample application depends" +
Fixed a code misquote in the "Implementation description for each use case" + 
Fixed parenthesis notation for tool names + 
Added alternate text to images

|2023/12/22
|Console Developer Edition and Console
Enterprise Edition support 

|2024/03/25
|AWS support

|===

== Introduction
This tutorial explains a sample application using the "**Cloud SDK**". + 
This sample application demonstrates the basic use of the "**Cloud SDK**". + 
In the sample application, one can verify how to control Edge Devices using "**Cloud SDK**" and also how to check the output of the Edge Devices uploaded to "**Console for AITRIOS**" (hereafter, referred to as "**Console**") or Azure Blob Storage or Amazon S3 or Local Storage. +
However, Codespaces cannot be used when checking the inference results uploaded to the Local Storage.

[#_precondition]
== Prerequisite
=== Connection information
To use the sample application, you need connection information to access the "**Console**" from the application. + 
The acquired information is used in <<#_Execute_sampleapp,"1.Prepare to run the sample application">>. + 
The required information is as follows:

* Client application details
- When "**Console Developer Edition**" is used
** Refer to the client application list in "**Portal for AITRIOS**" or register the client application for
the sample application based on the requirement and obtain the following information. For
details, refer to "Issuing a Client Secret for SDK" in the https://developer.aitrios.sony-semicon.com/en/edge-ai-sensing/documents/portal-user-manual/["**Portal User Manual**"].
*** Client ID
*** Secret
+
** Get the following information from link:++https://developer.aitrios.sony-semicon.com/en/file/download/edge-ai-sensing-portal-console-end-point-info/++["**Portal and Console endpoint information**"].
*** Console endpoint
*** Portal authorization endpoint

- When using "**Console Enterprise Edition**"
** Please contact "**Console**" deployment representative (Service Administrator).


- For using Azure Blob Storage, connection information is required to access Azure Blob Storage. +
For details, refer https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string#configure-a-connection-string-for-an-azure-storage-account["Configure a connection string for an Azure storage account"].

- To use Amazon S3, connection information is required to access Amazon S3.
** Fetch the below information by referring to https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#using_CreateAccessKey["AWS Identity and Access Management"].
*** Access key ID
*** Secret access key
+
NOTE: Use an account without MFA setting for external transfer settings or for obtaining AccessKey to be used in the application.

** Obtain the following information by referring to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html["User Guide for Linux instances"].
*** Region

** Create a bucket referring to https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html["Amazon Simple Storage Service (S3)"] and obtain the following information.
*** Bucket name


=== Edge Devices
In order for the sample application to work properly, the Edge Device must have specific settings. + 
Required settings are as follows:

* AI model and application are deployed
* AI model based on object detection is deployed
* From the "**Console**" UI, set the Command Parameter File to be used to the following setting:
+
** When using "**Console**" +
When not described, the following values are set automatically. +
UploadMethod="BlobStorage" +
UploadMethodIR="Mqtt" +
** When using Azure Blob Storage/Amazon S3 +
UploadMethod="BlobStorage" +
UploadMethodIR="BlobStorage" +
** Local Storage +
UploadMethod="HTTPStorage" +
UploadMethodIR="HTTPStorage" +
** All common upload locations +
NumberOfInferencesPerMessage=1 +
FileFormat="JPG" +
Mode=1 +
+
** Other parameters need to be changed depending on the AI model and application content

=== External transfer settings
* When using Azure Blob Storage +
When using Azure Blob Storage, complete the settings available in External transfer setting tutorial(Azure Blob Storage). +
* When using Amazon S3 +
When using Amazon S3, complete the settings in the External transfer settings tutorial (Amazon S3).
* When using Local Storage +
When using Local Storage, complete the settings available in the External transfer settings tutorial(Local HTTP Server).
+
IMPORTANT: Uploads from the Edge Device to HTTP Server are not encrypted due to HTTP communication.

== Sample application operating environment
See the https://developer.aitrios.sony-semicon.com/en/edge-ai-sensing/documents/sdk-getting-started/["**SDK Getting Started**"].

== Sample application functional overview
The sample application implements the functionality required to specify an Edge Device enrolled in the "**Console**" and get inference results and images. + 
The following three functions are implemented:

* Get information about Edge Devices enrolled in the "**Console**"
* Instruct Edge Devices to start/stop inference
+
By starting inference, the Edge Device uploads the inference results/images to "**Console**", Azure Blob Storage, Amazon S3 or Local Storage.
* Obtain the inference results/images and display the fetched results.
+
Displays the data uploaded to "**Console**" or Azure Blob Storage or Local Storage.


== Sample application repository structure
Sample application operating environment is as follows: + 
Omit parts that are not relevant to the implementation.
----
aitrios-sdk-cloud-app-sample-ts
├── src (1)
│   ├── common
│   │   └── config.ts (2)
│   │   └── settings.ts (3)
│   ├── components (4)
│   │   ├── Button
│   │   │   └── index.tsx
│   │   └── DropDownList
│   │       └── index.tsx
│   ├── hooks
│   │   └── getAwsStorage.ts (5)
│   │   └── getAzureStorage.ts (6)
│   │   └── getConsoleStorage.ts (7)
│   │   └── getLocalStorage.ts (8)
│   │   └── getStorageData.ts (9)
│   │   └── useInterval.js (10)
│   ├── next-env.d.ts
│   ├── pages
│   │   ├── api
│   │   │   ├── getCommandParameterFile.ts (11)
│   │   │   ├── getDeviceData.ts (12)
│   │   │   ├── getImageAndInference.ts (13)
│   │   │   ├── startUpload.ts (14)
│   │   │   └── stopUpload.ts (15)
│   │   ├── _app.tsx (16)
│   │   └── index.tsx (17)
│   ├── public
│   │   ├── favicon.ico (18)
│   │   └── label.json (19)
│   ├── styles
│   │   ├── globals.css (20)
│   │   └── Home.module.css (21)
│   ├── tsconfig.json (22)
│   └── util
│   │   ├── bounding-box.ts (23)
│   │   ├── bounding-box2d.ts (24)
│   │   ├── general-object.ts (25)
│   │   ├── object-detection-data.ts (26)
│   │   ├── object-detection-top.ts (27)
│   │   └── sample.ts (28)
│   └── checkLocalRoot.ts (29)
│   └── tsconfig.node.json  (30)
----
(1) src : Sample application folder +
(2) Config.ts : Logic to fetch the connection information to "**Console**" or Azure Blob Storage, Amazon S3 +
(3) settings.ts : Specify the path of Local Storage and connection destination +
(4) components : Button / DropDownList component logic storage folder +
(5) getAwsStorage.ts : Logic to obtain inference results and images from Amazon S3 +
(6) getAzureStorage.ts : Logic to fetch the inference results and images from Azure Blob Storage +
(7) getConsoleStorage.ts : Logic to fetch the inference results and images from "**Console**"  +
(8) getLocalStorage.ts : Logic to fetch the inference results and images from Local Storage +
(9) getStorageData.ts : Determines the Storage to use and invokes the logic of the appropriate Storage +
(10) useInterval.js : Interval logic +
(11) getCommandParameterFile.ts : Logic to get parameters for the Edge Device +
(12) getDeviceData.ts : Logic to get information about Edge Devices enrolled in the "**Console**" +
(13) getImageAndInference.ts : Logic to obtain the inference results and images +
(14) startUpload.ts : Logic to start inference +
(15) stopUpload.ts : Logic to stop inference +
(16) _app.tsx : Initializing the sample application frontend +
(17) index.tsx : Sample application frontend UI +
(18) favicon.ico : Symbol icons for the sample application +
(19) label.json : Inference result display label +
(20) globals.css : Sample application frontend style sheet +
(21) Home.module.css : Sample application frontend style sheet +
(22) tsconfig.json : Compiler configuration file +
(23) bounding-box.ts : Source code to deserialize +
(24) bounding-box2d.ts : Source code to deserialize +
(25) general-object.ts : Source code to deserialize +
(26) object-detection-data.ts : Source code to deserialize +
(27) object-detection-top.ts : Source code to deserialize +
(28) sample.ts : TypeScript logic running in the sample application frontend UI +
(29) checkLocalRoot.ts : Verify the LOCAL_ROOT settings +
(30) tsconfig.node.json : Compile settings file

=== Source code commentary

The following figure provides an overview of the sample application:

image::diagram_ts.png[alt="Overview of the sample application", width="400", align="center"]

The sample application consists of the Next.js framework.

Call the "**Cloud SDK**" from the sample application to control the Edge Device through the "**Console**". +
The data obtained by the Edge Device is saved in "**Console**", Azure Blob Storage, Amazon S3 or Local Storage. +
The sample application obtains data from "**Console**", Azure Blob Storage, Amazon S3 or Local Storage using "**Cloud SDK**", etc


=== Package (framework) on which the sample application depends

* "**Console Access Library**"
* https://nodejs.org/en/download/[node]
* https://github.com/axios/axios[axios]
* https://github.com/acode/lib-node[lib]
* https://nextjs.org/[next]
* https://reactjs.org/[react]
* https://reactjs.org/docs/react-dom.html[react-dom]
* https://google.github.io/flatbuffers/[FlatBuffers]
* https://github.com/nodeca/js-yaml[js-yaml]
* https://github.com/Azure/azure-sdk-for-js[azure-sdk-for-js]
* https://aws.amazon.com/sdk-for-javascript/[AWS SDK for JavaScript]


[#_Execute_sampleapp]
== How to run the sample application
Use the connection information prepared in the <<#_precondition,"Prerequisite">>

=== 1.Prepare to run the sample application
. In Codespaces or in an environment where the repository is cloned, create
[console_access_settings.yaml] under [src/common] and set the connection destination information.

- When "**Console Developer Edition**" is used
+
|===
|src/common/console_access_settings.yaml
a|
[source, Yaml]
----
console_access_settings:
  console_endpoint: "Console endpoint"
  portal_authorization_endpoint: "Portal authorization endpoint"
  client_secret: "Secret"
  client_id: "Client ID"
----
|===
* Specify the Console endpoint in the `**console_endpoint**`. +
* Specify the Portal authentication endpoint in `**portal_authorization_endpoint**`. +
* Specify the Secret of the registered application in `**client_secret**`. +
* Specify the Client ID of the registered application in the `**client_id**`. +
+

IMPORTANT: For details on how to obtain the Client ID and Secret, please refer "Issue the Client Secret for SDK" in the https://developer.aitrios.sony-semicon.com/en/edge-ai-sensing/documents/portal-user-manual/["**Portal User Manual**"]. +
For details on how to obtain the Console endpoint and the Portal authentication endpoint, please refer to link:++https://developer.aitrios.sony-semicon.com/en/file/download/edge-ai-sensing-portal-console-end-point-info/++[this document]. +
This is the information to access the "Console". +
Do not disclose it to the public or share it with others and handle it with caution.
+

NOTE: When executing the sample application in a Proxy environment set the
environment variable `**https_proxy**`. +

- When "**Console Enterprise Edition**" is used
+ 
|===
|src/common/console_access_settings.yaml
a|
[source,Yaml]
----
console_access_settings:
  console_endpoint: "Console endpoint"
  portal_authorization_endpoint: "Portal authentication endpoint" 
  client_secret: "Secret"
  client_id:  "Client ID" 
  application_id: "Application ID"
----
|===
+ 

* Specify the Console endpoint in `**console_endpoint**`. +
* Specify the Portal authentication endpoint in `**portal_authorization_endpoint**`. +
The Portal authentication endpoint is to be specified in a `**\https://login.microsoftonline.com/{tenantID}**` format. +
* Specify the Secret of the registered application in `**client_secret**` . +
* Specify the Client ID of the registered application in the `**client_id**`. +
* Specify the Application ID of the registered application in `**application_id**`.
+

IMPORTANT: For details on how to fetch the Console endpoint, Client ID, Secret and Tenant ID and Application ID, please contact "**Console**" deployment representative (Service Administrator). +
Do not disclose it to the public or share it with others, handle it with care. +
+

NOTE: When executing the sample application in a Proxy environment, set the
environment variable `**https_proxy**`.

. In Codespaces or in an environment where the repository is cloned, create
[azure_access_settings.yaml] under [src/common] and set the connection destination information. +
This setting is set when the destination to obtain the inference results is Azure Blob Storage.

+ 
|===
|src/common/azure_access_settings.yaml
a|
[source,Yaml]
----
azure_access_settings:
  connection_string: "Connection information"
  container_name: "Container name".
----
|===
+ 

* Specify the Connection information of Azure Blob Storage in `**connection_string**`.
* Specify the Container name of Azure Blob Storage in `**container_name**`.

+ 
IMPORTANT: This is the information to access the Azure Blob Storage. +
Do not disclose it to the public or share it with others and handle it
with caution.

. Create [aws_access_settings.yaml] under [src/common] in the environment where the repository is
cloned or in Codespaces and set the connection destination information. +
This setting is set when the destination to obtain the inference results is Amazon S3.

+
|===
|src/common/aws_access_settings.yaml
a|
[source,Yaml]
----
aws_access_settings:
  bucket_name: "Bucket name"
  access_key_id: "Access KeyID"
  secret_access_key: "Secret access key"
  region: "Region"
----
|===
+
* Set `**bucket_name**` to the Amazon S3 bucket name. +
* In `**access_key_id**`, specify the access key ID of Amazon S3. +
* Specify the Amazon S3 secret access key in `**secret_access_key**`. +
* In `**region**`, specify the Amazon S3 region. +
+

IMPORTANT: These are the access information to Amazon S3. +
Do not disclose it to the public or share it with others and handle it with care. +


. In Codespaces or in an environment where the repository is cloned, set the connection destination information in [settings.ts] under [src/common].

+ 
|===
|src/common/settings.ts
a|
[source,TypeScript]
----
export const SERVICE = {
  Console: 'console',
  Azure: 'azure',
  AWS: 'aws',
  Local: 'local'
} as const
type SERVICE_TYPE = typeof SERVICE[keyof typeof SERVICE];

export const CONNECTION_DESTINATION: SERVICE_TYPE = SERVICE.Console
export const LOCAL_ROOT = ''
----
|===
+ 

* Set the destination to obtain the inference result in `**CONNECTION_DESTINATION**`. +
The default value is the `**SERVICE.Console**` setting.

* Specify the path for Local Storage `**LOCAL_ROOT**`.
This setting is used when `**SERVICE.Local**` is specified in `**CONNECTION_DESTINATION**`.

+ 
NOTE: When using Dev Container environment, create a folder in the folder where Local Storage is
git cloned and set `**LOCAL_ROOT**` to `**workspace/{ folder that is created within a git cloned folder}**`.

image::prepare_ts.png[alt="Prepare to run the sample application", width="700", align="center"]

=== 2.Launch the sample application
Install the package and launch the sample application from either the terminal in the environment where the repository is cloned or from Codespaces.
 
....
$ npm install
$ npm run build
$ npm run start
....

image::launch_app_ts.png[alt="Launch the sample application", width="700", align="center"]

=== 3.Start inference
Access the sample application from the browser and perform various operations.

. Open http://localhost:3000 (port forwarded URL in the case of Codespaces).
. Select a Device ID from the list of [**DeviceID**]
. Click the [**START**] to start inference for the Edge Device

image::start_inference_ts.png[alt="Start inference", width="700", align="center"]

=== 4.Review inference results and images
While inference is starting, the "**Image/Inference**" area displays an image and inference results.

image::running_ts.png[alt="Review inference results and images", width="700", align="center"]


=== 5.Stop inference
Click the [**STOP**] in the sample application to stop inference for the Edge Device.

image::stop_inference_ts.png[alt="Stop inference", width="700", align="center"]

== Implementation description for each use case

=== 1.Get information about Edge Devices enrolled in the "**Console**"

To use the "**Console**", generate a Client for the "**Cloud SDK**". + 
Use the functions provided by the "**Console**" from the generated Client.

* Import library 
+
[source, TypeScript]
----
import { Client, Config } from 'consoleAccessLibrary'
----
+
Import the libraries required for "**Cloud SDK**" client generation, as preceding.

* "**Cloud SDK**" client generation
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
----
In the preceding source code, generate the client for the "**Cloud SDK**". + 
Specify the connection information to the `**Config**` and generate the `**config**`. + 
Specify the `**config**` to the `**Client**` and generate the `**client**`.

* Get Edge Device information
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const res = await client?.deviceManagement?.getDevices(queryParams)
----
In the preceding example, get information about the enrolled Edge Devices from the "**Console**". + 
Use the generated client and run the `**getDevices**` provided by the `**deviceManagement**` to get Edge Device information. + 
Optionally acquisition conditions is configurable to the `**queryParams**`.

* Get Edge Device parameters
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const res = await client?.deviceManagement?.getCommandParameterFile()
----
Generate the `**client**` as preceding. + 
Get Edge Device parameters using the `**getCommandParameterFile**` provided by the `**deviceManagement**` of the `**client**`.

=== 2.Instruct the Edge Devices to start inference

* Start inference
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const res = await client?.deviceManagement?.startUploadInferenceResult(deviceId)
----
Generate the `**client**` as preceding. + 
Start inference using the `**startUploadInferenceResult**` provided by the `**deviceManagement**` of the `**client**`.

=== 3.Get inference results and images from the "**Console**"
Use the functionality provided by client to get inference results and images from the "**Console**".

* Get an image list
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const imageData = await client?.insight?.getImages(deviceId, subDirectoryName, numberOfImages, skip, orderBy)
----
Generate the `**client**` as preceding. + 
Get the image list using the `**getImages**` provided by the `**insight**`.

* Get the latest image and link it to the inference result
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const orderBy = 'DESC'
const numberOfImages = 1
const skip = 0
const imageData = await client?.insight?.getImages(deviceId, outputSubDir, numberOfImages, skip, orderBy)
const latestImage = imageData.data.images[0]
const ts = (latestImage.name).replace('.jpg', '')
const base64Img = `data:image/jpg;base64,${latestImage.contents}`
----
The preceding source code gets the latest image information from an image list. + 
Get the latest image data into the `**base64Img**`. + 
Get the timestamp of the latest image into the `**ts**`. + 
Inference results and images are linked by their respective timestamps. + 
Call the function to get inference results linked to images using the `**ts**`.

* Get inference results linked to the latest image
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret, application_id)
const client = await Client.createInstance(config)
const filter = `EXISTS(SELECT VALUE i FROM i IN c.Inferences WHERE i.T >= "${startTime}" AND i.T <= "${endTime}")`
const NumberOfInferenceResults = 1
const raw = 1
const time = undefined
const resInference = await client.insight.getInferenceResults(deviceId, filter, numberOfInferenceResult, raw, time)
----
Generate the `**client**` as preceding. + 
Get the list of inference results using the `**getInferenceResults**` provided by the `**insight**`. +
`**filter**` is the argument that specifies a search filter. + 
Specify the number of inference results to get by the `**numberOfInferenceResult**`. +
`**raw**` is the argument for accessing the stored inference result. + 
Specify the timestamp of inference results to get by the `**time**`.

* Deserialize inference results
+
[source, TypeScript]
----
const deserializedInferenceData = deserialize(inferenceData)
----
The preceding source code converts the inference results gotten from the "**Console**" into a format that can be referenced. + 
See the https://github.com/SonySemiconductorSolutions/aitrios-sdk-deserialization-sample["Cloud SDK Deserialize Sample"] for details of this conversion process.


=== 4.Obtain the inference results and images of Azure Blob Storage

In order to obtain the inference results and images from Azure Blob Storage, use
getAzureStorage.ts available in the hooks directory.

* Obtain the image list
+
[source,TypeScript]
----
export async function getImageFromAzure (deviceId: string, subDirectory: string, orderBy?: string, skip?: number, numberOfImages?: number) {
  const containerClient = getBlobService()
  const blobNames = []
  const prefix = `${deviceId}/image/${subDirectory}/`
  orderBy = orderBy || 'ASC' // ASC is cal default value
  skip = skip || 0 // 0 is cal default value
  numberOfImages = numberOfImages || 50 // 50 is cal default value
  for await (const blob of containerClient.listBlobsFlat({ prefix })) {
    blobNames.push(blob.name)
  }
  if (orderBy === 'DESC') {
    blobNames.reverse()
  }

  const images = []
  for (let i = 0; i < blobNames.length; i++) {
    if (i === numberOfImages) break
    if (blobNames[i + skip] === undefined) {
      break
    }
    const blockBlobClient = containerClient.getBlockBlobClient(blobNames[i + skip])
    const buffer = await blockBlobClient.downloadToBuffer()
    images.push({
      name: blobNames[i + skip].split('/')[3],
      contents: buffer.toString('base64')
    })
  }

  const response = {
    total_image_count: blobNames.length,
    images
  }
  return response
}
----

Obtain the list of image file names using `**listBlobsFlat**` provided by `**azure-sdk-for-js**`. +
Obtain image data by using `**getBlockBlobClient**` and `**downloadToBuffer**` by using `**azure-sdk-for-js**`.  +
Creates an image file name, base64, and returns it together with `**total_image_count**`

* Obtains the inference result associated with the latest image
+
[source,TypeScript]
----
export async function getInferenceFromAzure (retryCount: number, deviceId: string, subDirectory: string, startInferenceTime?: string, endInferenceTime?: string, numberOfInferenceResult?: number): Promise<string[]> {
  const serializeDatas: string[] = []
  if (retryCount === 0) {
    return serializeDatas
  }
  const containerClient = getBlobService()
  const blobs = []
  numberOfInferenceResult = numberOfInferenceResult || 20 // 20 is cal default value
  const prefix = `${deviceId}/metadata/${subDirectory}/`

  for await (const blob of containerClient.listBlobsByHierarchy('/', { prefix })) {
    const filePath = blob.name
    const timestamp = filePath.split('/')[3].replace('.txt', '')
    if ((startInferenceTime === undefined || timestamp >= startInferenceTime) &&
      (endInferenceTime === undefined || timestamp < endInferenceTime)) {
      blobs.push(blob.name)
    } else if (endInferenceTime !== undefined && timestamp > endInferenceTime) {
      break
    }
    if (blobs.length === numberOfInferenceResult) break
  }

  if (!(blobs.length === 0)) {
    for (let i = 0; i < blobs.length; i++) {
      const blobClient = containerClient.getBlobClient(blobs[i])
      const blobInferenceResponse = await blobClient.download(0)
      const inferenceText = await streamToString(blobInferenceResponse.readableStreamBody)
      const inferenceJson = JSON.parse(inferenceText)
      serializeDatas.push(inferenceJson)
    }
    return serializeDatas
  } else {
    await setTimeout(1000)
    return getInferenceFromAzure(retryCount - 1, deviceId, subDirectory, startInferenceTime, endInferenceTime, numberOfInferenceResult)
  }
}
----

Obtains the list of inference result file names using `**listBlobsByHierarchy**` provided by `**azure-sdk-for-js**`. +
Check whether the time stamp of the obtained inference result file name is within the specified
range. +
Obtain the inference result data using `**getBlobClient**` or `**download**` provided by `**azure-sdk-for-js**`. +
`**startInferenceTime**` is a time stamp that indicates the search start position. +
`**endInferenceTime**` is a time stamp that indicates the search end position. +
`**numberOfInferenceResult**` is the number of inference results to be obtained. 

=== 5. Obtain the inference results and images of Amazon S3
In order to obtain the inference results and images from Amazon S3, getAwsStorage is used that is available under the storage directory.

* Obtain the image list
+
[source,TypeScript]
----
async function getListObjects (deviceId: string, subDirectory: string, client: S3Client, listObjInput: ListObjectsCommandInput, resultList?: string[]): Promise<any> {
  const listObjCommand = new ListObjectsV2Command(listObjInput)
  const listObjData = await client.send(listObjCommand)
  const returnList = resultList || []

  if (listObjData !== undefined && listObjData.Contents !== undefined) {
    for (const content of listObjData.Contents) {
      const key = JSON.stringify(content)
      if (key !== undefined) {
        returnList.push(JSON.parse(key))
      }
    }
    if (listObjData.IsTruncated) {
      const input = { ...listObjInput, StartAfter: listObjData.Contents.slice(-1)[0].Key }
      return getListObjects(deviceId, subDirectory, client, input, returnList)
    }
  }
  return returnList
}
----
Obtain the list of objects in the bucket using `**ListObjectsV2Command**` provided by `**AWS SDK for JavaScript**`. +
Obtains the key of the obtained object and returns it in the list.

[source,TypeScript]
----
export async function getImageFromAws (retryCount: number, deviceId: string, subDirectory: string, orderBy?: string, skip?: number, numberOfImages?: number): Promise<getImageFromAwsResult> {
  const response: getImageFromAwsResult = {
    total_image_count: 0,
    images: []
  }
  const { client, bucket } = await getS3Client()
  const listObjInput = {
    Bucket: bucket,
    Prefix: `${deviceId}/image/${subDirectory}`
  }
  orderBy = orderBy || 'ASC' // ASC is default value
  skip = skip || 0 // 0 is default value
  numberOfImages = numberOfImages || 50

  const listObjData = await getListObjects(deviceId, subDirectory, client, listObjInput)
  const images: getImageFromAwsResult['images'] = []
  if (listObjData.length !== 0) {
    if (orderBy === 'DESC') {
      listObjData.reverse()
    }
    for (let i = 0; i < listObjData.length; i++) {
      if (i === numberOfImages) break
      if (listObjData[i + skip] === undefined || listObjData[i + skip].Key === undefined) break
      const fileName = listObjData[i + skip].Key
      const getObjInput = {
        Bucket: bucket,
        Key: fileName
      }
      const command = new GetObjectCommand(getObjInput)
      const imageData = await client.send(command)
      if (fileName !== undefined && imageData !== undefined && imageData.Body !== undefined) {
        images.push({
          name: fileName.split('/')[3],
          contents: await imageData.Body.transformToString('base64')
        })
      }
    }
    if (images.length !== 0) {
      response.total_image_count = listObjData.length
      response.images = images
      return response
    }
  }
  if (retryCount > 0) {
    await setTimeout(1000)
    return getImageFromAws(retryCount - 1, deviceId, subDirectory, orderBy, skip, numberOfImages)
  }
  return response
}
----
Obtains the object using `**GetObjectCommand**` provided by `**AWS SDK for JavaScript**` +
Creates an image file name and base64 and returns it together with `**total_image_count**`.

* Obtains the inference result associated with the latest image
[source,TypeScript]
+
----
export async function getInferenceFromAws (retryCount: number, deviceId: string, subDirectory: string, startInferenceTime?: string, endInferenceTime?: string, numberOfInferenceResult?: number): Promise<string[]> {
  const { client, bucket } = await getS3Client()
  const serializeData: string[] = []
  const listObjInput = {
    Bucket: bucket,
    Prefix: `${deviceId}/metadata/${subDirectory}`
  }
  numberOfInferenceResult = numberOfInferenceResult || 20
  const listObjData = await getListObjects(deviceId, subDirectory, client, listObjInput)
  if (listObjData.length !== 0) {
    for (let i = 0; i < listObjData.length; i++) {
      if (i === numberOfInferenceResult) break
      const key = listObjData[i].Key
      if (key !== undefined) {
        const timestamp = path.parse(key).name
        if ((startInferenceTime === undefined || timestamp >= startInferenceTime) &&
          (endInferenceTime === undefined || timestamp < endInferenceTime)) {
          const getObjInput = {
            Bucket: bucket,
            Key: key
          }
          const command = new GetObjectCommand(getObjInput)
          const inferenceData = await client.send(command)
          if (key !== undefined && inferenceData !== undefined && inferenceData.Body !== undefined) {
            const inferenceText = await inferenceData.Body.transformToString()
            if (inferenceText !== undefined) {
              const inferenceJson = JSON.parse(inferenceText)
              serializeData.push(inferenceJson)
            }
          }
        } else if (endInferenceTime !== undefined && timestamp > endInferenceTime) {
          break
        }
      }
    }
  }
  if (serializeData.length !== 0) {
    return serializeData
  }
  if (retryCount > 0) {
    await setTimeout(1000)
    return getInferenceFromAws(retryCount - 1, deviceId, subDirectory, startInferenceTime, endInferenceTime, numberOfInferenceResult)
  }
  return serializeData
}
----
Obtains the object using `**GetObjectCommand**` provided by `**AWS SDK for JavaScript**` +
`**startInferenceTime**` is a timestamp that indicates the search start position. +
`**endInferenceTime**` is a timestamp that indicates the search end position. +
`**numberOfInferenceResult**` is the number of inference results to obtain. +

=== 6.Obtains the inference results and images of Local Storage
In order to obtain the inference results and images from Local Storage, use LocalStorage.ts
available in the hooks directory.

* Obtain the image list
+
[source,TypeScript]
----
export function getImageFromLocal (deviceId: string, subDirectory: string, orderBy?: string, skip?: number, numberOfImages?: number) {
  const storagePath = path.join(LOCAL_ROOT, deviceId, 'image', subDirectory)
  isRelativePath(storagePath)
  orderBy = orderBy || 'ASC' // ASC is cal default value
  skip = skip || 0 // 0 is cal default value
  numberOfImages = numberOfImages || 50 // 50 is cal default value
  const images: any = []
  isStoragePathFile(storagePath)
  const files = fs.readdirSync(storagePath)
  const imagesFiles = files.filter(file => {
    const extension = path.extname(file).toLowerCase()
    return extension === '.jpg'
  })
  if (orderBy === 'DESC') {
    imagesFiles.reverse()
  }
  for (let i = 0; i < numberOfImages; i++) {
    if (imagesFiles[i + skip] === undefined) {
      break
    }
    const filePath = path.join(storagePath, imagesFiles[i + skip])
    isRelativePath(filePath)
    isSymbolicLinkFile(filePath)
    const data = fs.readFileSync(filePath)
    const base64Image = base64.fromByteArray(data)
    images.push({
      name: imagesFiles[i + skip],
      contents: base64Image
    })
  }
  const response = {
    total_image_count: imagesFiles.length,
    images
  }
  return response
}
----

Obtains the list of image file names using `**readdirSync**` provided by `**fs**`. +
Obtains the image data using `**readFileSync**` provided by `**fs**`. +
Creates an image file name, base64, and returns it together with `**total_image_count**`. +

* Obtains the inference result associated with the latest image +
+
[source,TypeScript] 
----
export function getInferenceFromLocal (deviceId: string, subDirectory: string, startInferenceTime?: string, endInferenceTime?: string, numberOfInferenceResult?: number) {
  const storagePath = path.join(LOCAL_ROOT, deviceId, 'meta', subDirectory)
  isRelativePath(storagePath)
  numberOfInferenceResult = numberOfInferenceResult || 20 // 20 is cal default value
  isStoragePathFile(storagePath)
  const serializeDatas: string[] = []
  const inferencesFiles = fs.readdirSync(storagePath) // get inferences
  for (const fileName of inferencesFiles) {
    const timestamp = path.basename(fileName, '.txt')
    if ((startInferenceTime === undefined || timestamp >= startInferenceTime) &&
      (endInferenceTime === undefined || timestamp < endInferenceTime)) {
      const inferenceFilePath = path.join(LOCAL_ROOT, deviceId, 'meta', subDirectory, fileName)
      isSymbolicLinkFile(inferenceFilePath)
      const inferenceData = fs.readFileSync(inferenceFilePath, 'utf8')
      const json = JSON.parse(inferenceData)
      serializeDatas.push(json)
    } else if (endInferenceTime !== undefined && timestamp > endInferenceTime) {
      break
    }
    if (serializeDatas.length === numberOfInferenceResult) break
  }

  return serializeDatas
}
----

Obtains the list of inference result file names using `**readdirSync**` provided by `**fs**`. +
Check whether the time stamp of the obtained inference result file name is within the specified range. +
Obtains the inference result data using `**readFileSync**` provided by `**fs**`. +
`**startInferenceTime**` is a time stamp that indicates the search start position. +
`**endInferenceTime**` is a time stamp that indicates the search end position. +
`**numberOfInferenceResult**` is the number of inference results to be obtained.

=== 7.Instruct the Edge Devices to stop inference

* Stop inference
+
[source, TypeScript]
----
const config = new Config(console_endpoint, portal_authorization_endpoint, client_id, client_secret);
const client = await Client.createInstance(config)
const res = await client?.deviceManagement?.stopUploadInferenceResult(deviceId)
----
To stop inference of the Edge Device, run the `**stopUploadInferenceResult**` provided by the `**deviceManagement**` of the `**client**` as preceding. + 
Specify the Device ID to stop by the `**deviceId**`.

== Reference materials

=== Display gotten inference results (Sample application display processing)

[source, JavaScript]
----
type InferenceItem = {
  'class_id': number, // Index of the object label
  'score': number,    // Confidence of the object label
  'left': number,     // X-coordinate start position of the object
  'top': number,      // Y coordinate start position of the object
  'right': number,    // X-coordinate end position of the object
  'bottom': number    // Y coordinate end position of the object
}
const drawBoundingBox = (image, inferenceData, context, labels) => {
  if (context !== null) {
    const img = new window.Image()
    img.src = image
    img.onload = () => {
      const canvas = document.getElementById('canvas') as HTMLCanvasElement
      canvas.width = img.width
      canvas.height = img.height
      context.drawImage(img, 0, 0)

      // Display gotten inference results
      for (const [key, value] of Object.entries(inferenceData)) {
        if (key === 'T') continue
        const v = value as InferenceItem
        context.lineWidth = 3
        context.strokeStyle = 'rgb(255, 255, 0)'

        // Specify bounding box coordinates
        context.strokeRect(v.left, v.top, Math.abs(v.left - v.right), Math.abs(v.top - v.bottom))

        // Specify coordinates to display labels
        const labelPointX = (v.right > 270 ? v.right - 70 : v.right)
        const labelPointY = (v.bottom > 300 ? v.bottom - 10 : v.bottom)

        context.font = '20px Arial'
        context.fillStyle = 'rgba(255, 255, 0)'

        // Display the label and confidence
        context.fillText(`${labels[v.class_id]} ${Math.round(v.score * 100)}%`, labelPointX, labelPointY)
      }
    }
  }
}
----

* Format of image paths taken as an image list
+
----
<blobcontainer_name>/<deviceId>/JPG/<subDirectoryName>/YYYYMMDDHHMMSSFFF.jpg
----
* Sample data of inference result (object detection) + 
Inferences[] is the inference result + 
In the following sample data, there are two object detections + 
The detection results are serialized, but the following sample data is in deserialized data format.
+
[source, Json]
----
{
    "DeviceID": "123456789ABC",
    "ModelID": "0000000000000000",
    "Image": true,
    "Inferences": [
        {
            "1": {
                "class_id": 18,
                "score": 0.03125,
                "left": 8,
                "top": 0,
                "right": 303,
                "bottom": 107
            },
            "2": {
                "class_id": 19,
                "score": 0.02734375,
                "left": 2,
                "top": 230,
                "right": 38,
                "bottom": 319
            },
            "T": "20220101010101000"
        }
    ],
    "id": "00000000-0000-0000-0000-000000000000",
    "_rid": "AAAAAAAAAAAAAAAAAAAAAA==",
    "_self": "dbs/XXXXXX==/colls/CCCCCCCCCCCC=/docs/AAAAAAAAAAAAAAAAAAAAAA==/",
    "_etag": "\"00000000-0000-0000-0000-000000000000\"",
    "_attachments": "attachments/",
    "_ts": 1111111111
}
----
+
The parameters of the detection result are as follows:
+
class_id: Index of the object label
+
score: Confidence of the object label
+
left: X-coordinate start position of the object
+
top: Y coordinate start position of the object
+
right: X-coordinate end position of the object
+
bottom: Y coordinate end position of the object
